{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and parse data from all files\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' : ')\n",
    "                if len(parts) == 4:\n",
    "                    event_type, diagram, amplitude, squared_amplitude = parts\n",
    "                    data.append({\n",
    "                        'event_type': event_type,\n",
    "                        'diagram': diagram,\n",
    "                        'amplitude': amplitude.strip(),\n",
    "                        'squared_amplitude': squared_amplitude.strip()\n",
    "                    })\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_indices(expr):\n",
    "    # Find all patterns like %something_number\n",
    "    pattern = r'(%[^_]+)_(\\d+)'\n",
    "    matches = re.findall(pattern, expr)\n",
    "    \n",
    "    # Get unique numeric indices for each variable type\n",
    "    var_indices = {}\n",
    "    for var_type, num_idx in matches:\n",
    "        if var_type not in var_indices:\n",
    "            var_indices[var_type] = set()\n",
    "        var_indices[var_type].add(num_idx)\n",
    "    \n",
    "    # Create mapping from original indices to normalized ones for each variable type\n",
    "    index_maps = {}\n",
    "    for var_type, indices in var_indices.items():\n",
    "        sorted_indices = sorted(indices, key=int)\n",
    "        index_maps[var_type] = {orig_idx: str(i+1) for i, orig_idx in enumerate(sorted_indices)}\n",
    "    \n",
    "    # Replace indices according to the mapping\n",
    "    def replace_match(m):\n",
    "        var_type, num_idx = m.groups()\n",
    "        return f\"{var_type}_{index_maps[var_type][num_idx]}\"\n",
    "    \n",
    "    normalized_expr = re.sub(pattern, replace_match, expr)\n",
    "    \n",
    "    return normalized_expr\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for mathematical expressions\n",
    "def tokenize_expression(expr):\n",
    "    # Define patterns for different token types\n",
    "    patterns = [\n",
    "        # Constants and numbers\n",
    "        r'(\\d+/\\d+|\\d+\\.\\d+|\\d+)',\n",
    "        # Variables with indices and special notations\n",
    "        r'([a-zA-Z]+(?:_[a-zA-Z0-9]+)?(?:\\^\\([*]\\))?)',\n",
    "        # Mathematical operators and symbols\n",
    "        r'([\\+\\-\\*/\\^\\(\\)\\[\\]\\{\\}])',\n",
    "        # Special sym|bols and groupings\n",
    "        r'(%[a-zA-Z]+_\\d+|_{[^}]+})'\n",
    "    ]\n",
    "    \n",
    "    # Combine patterns\n",
    "    combined_pattern = '|'.join(patterns)\n",
    "    tokens = re.findall(combined_pattern, expr)\n",
    "    \n",
    "    # Flatten and filter empty strings\n",
    "    tokens = [t for sublist in tokens for t in sublist if t]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original expression: -1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_146}*gamma_{%\\sigma_165,%gam_147,%del_137}*e_{i_3,%gam_146}(p_1)_u*e_{k_3,%del_137}(p_2)_u*e_{l_3,%gam_145}(p_3)_u^(*)*e_{i_5,%gam_147}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\n",
      "Tokenized expression: ['-', '1/2', '*', 'i', '*', 'e', '^', '2', '*', 'gamma', '_{+%\\\\sigma_165,%gam_145,%gam_146}', '*', 'gamma', '_{%\\\\sigma_165,%gam_147,%del_137}', '*', 'e', '_{i_3,%gam_146}', '(', 'p_1', ')', 'u', '*', 'e', '_{k_3,%del_137}', '(', 'p_2', ')', 'u', '*', 'e', '_{l_3,%gam_145}', '(', 'p_3', ')', 'u^(*)', '*', 'e', '_{i_5,%gam_147}', '(', 'p_4', ')', 'u^(*)', '/', '(', 'm_e', '^', '2', '+', '-', 's_13', '+', '1/2', '*', 'reg_prop', ')']\n"
     ]
    }
   ],
   "source": [
    "# Example of how the tokenization works\n",
    "example_expr = \"-1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_146}*gamma_{%\\sigma_165,%gam_147,%del_137}*e_{i_3,%gam_146}(p_1)_u*e_{k_3,%del_137}(p_2)_u*e_{l_3,%gam_145}(p_3)_u^(*)*e_{i_5,%gam_147}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\"\n",
    "tokens = tokenize_expression(example_expr)\n",
    "print(\"Original expression:\", example_expr)\n",
    "print(\"Tokenized expression:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokennize using tokeninzer\n",
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "#special_tokens = ['*', '/', '+', '-', '^', '(', ')', '{', '}', '_', 'gamma', 'sigma', 'e^2']\n",
    "#tokenizer.add_tokens(special_tokens)\n",
    "#tokenizer.add_tokens(['[START]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original expression: -1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_146}*gamma_{%\\sigma_165,%gam_147,%del_137}*e_{i_3,%gam_146}(p_1)_u*e_{k_3,%del_137}(p_2)_u*e_{l_3,%gam_145}(p_3)_u^(*)*e_{i_5,%gam_147}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\n",
      "Tokenized expression: ['▁', '-', '1/2', '*', 'i', '*', 'e', '^', '2', '*', 'gam', 'm', 'a', '_', '{', '+', '%', '\\\\', 's', 'igma', '_', '165', ',', '%', 'gam', '_', '145', ',', '%', 'gam', '_', '146', '}', '*', 'gam', 'm', 'a', '_', '{', '%', '\\\\', 's', 'igma', '_', '165', ',', '%', 'gam', '_', '147', ',', '%', 'de', 'l', '_', '137', '}', '*', 'e', '_', '{', 'i', '_', '3,', '%', 'gam', '_', '146', '}', '(', 'p', '_', '1)', '_', 'u', '*', 'e', '_', '{', 'k', '_', '3,', '%', 'de', 'l', '_', '137', '}', '(', 'p', '_', '2)', '_', 'u', '*', 'e', '_', '{', 'l', '_', '3,', '%', 'gam', '_', '145', '}', '(', 'p', '_', '3)', '_', 'u', '^', '(', '*', ')', '*', 'e', '_', '{', 'i', '_', '5,', '%', 'gam', '_', '147', '}', '(', 'p', '_', '4)', '_', 'u', '^', '(', '*', ')', '/', '(', 'm', '_', 'e', '^', '2', '▁+', '▁', '-', 's', '_', '13', '▁+', '▁1/2', '*', 're', 'g', '_', 'prop', ')']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function for mathematical expressions\n",
    "def tokenize_expression(expr):\n",
    "    # Tokenize the expression\n",
    "    tokens = tokenizer.tokenize(expr)\n",
    "    return tokens\n",
    "\n",
    "# Example of how the tokenization works\n",
    "tokens = tokenize_expression(example_expr)\n",
    "print(\"Original expression:\", example_expr)\n",
    "print(\"Tokenized expression:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer for processing symbolic mathematical expressions.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, index_token_pool_size, momentum_token_pool_size, special_symbols, UNK_IDX, to_replace):\n",
    "        self.amps = df.amplitude.tolist()\n",
    "        self.sqamps = df.squared_amplitude.tolist()\n",
    "\n",
    "        # # Issue warnings if token pool sizes are too small\n",
    "        # if index_token_pool_size < 100:\n",
    "        #     warnings.warn(f\"Index token pool size ({index_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        # if momentum_token_pool_size < 100:\n",
    "        #     warnings.warn(f\"Momentum token pool size ({momentum_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        \n",
    "        # Generate token pools\n",
    "        self.tokens_pool = [f\"INDEX_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.momentum_pool = [f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size)]\n",
    "        \n",
    "        # Regular expression patterns for token replacement\n",
    "        self.pattern_momentum = re.compile(r'\\b[ijkl]_\\d{1,}\\b')\n",
    "        self.pattern_num_123 = re.compile(r'\\b(?![ps]_)\\w+_\\d{1,}\\b')\n",
    "        self.pattern_special = re.compile(r'\\b\\w+_+\\w+\\b\\\\')\n",
    "        self.pattern_underscore_curly = re.compile(r'\\b\\w+_{')\n",
    "        self.pattern_prop = re.compile(r'Prop')\n",
    "        self.pattern_int = re.compile(r'int\\{')\n",
    "        self.pattern_operators = {\n",
    "            '+': re.compile(r'\\+'), '-': re.compile(r'-'), '*': re.compile(r'\\*'),\n",
    "            ',': re.compile(r','), '^': re.compile(r'\\^'), '%': re.compile(r'%'),\n",
    "            '}': re.compile(r'\\}'), '(': re.compile(r'\\('), ')': re.compile(r'\\)')\n",
    "        }\n",
    "        self.pattern_mass = re.compile(r'\\b\\w+_\\w\\b')\n",
    "        self.pattern_s = re.compile(r'\\b\\w+_\\d{2,}\\b')\n",
    "        self.pattern_reg_prop = re.compile(r'\\b\\w+_\\d{1}\\b')\n",
    "        self.pattern_antipart = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)\\^\\(\\*\\)')\n",
    "        self.pattern_part = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)')\n",
    "        self.pattern_index = re.compile(r'\\b\\w+_\\w+_\\d{2,}\\b')\n",
    "        \n",
    "        self.special_symbols = special_symbols\n",
    "        self.UNK_IDX = UNK_IDX\n",
    "        self.to_replace = to_replace\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_whitespace(expression):\n",
    "        \"\"\"Remove all forms of whitespace from the expression.\"\"\"\n",
    "        return re.sub(r'\\s+', '', expression)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_expression(expression):\n",
    "        \"\"\"Split the expression by space delimiter.\"\"\"\n",
    "        return re.split(r' ', expression)\n",
    "\n",
    "    def build_tgt_vocab(self):\n",
    "        \"\"\"Build vocabulary for target sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for eqn in tqdm(self.sqamps, desc='Processing target vocab'):\n",
    "            counter.update(self.tgt_tokenize(eqn))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "\n",
    "    def build_src_vocab(self, seed):\n",
    "        \"\"\"Build vocabulary for source sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for diag in tqdm(self.amps, desc='Processing source vocab'):\n",
    "            counter.update(self.src_tokenize(diag, seed))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "    \n",
    "    def src_replace(self, ampl, seed):\n",
    "        \"\"\"Replace indexed and momentum variables with tokenized equivalents.\"\"\"\n",
    "        ampl = self.remove_whitespace(ampl)\n",
    "        \n",
    "        random.seed(seed)\n",
    "        token_cycle = cycle(random.sample(self.tokens_pool, len(self.tokens_pool)))\n",
    "        momentum_cycle = cycle(random.sample(self.momentum_pool, len(self.momentum_pool)))\n",
    "        \n",
    "        # Replace momentum tokens\n",
    "        temp_ampl = ampl\n",
    "        momentum_mapping = {match: next(momentum_cycle) for match in set(self.pattern_momentum.findall(ampl))}\n",
    "        for key, value in momentum_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "        \n",
    "        # Replace index tokens\n",
    "        num_123_mapping = {match: next(token_cycle) for match in set(self.pattern_num_123.findall(ampl))}\n",
    "        for key, value in num_123_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "\n",
    "        # Replace pattern index tokens\n",
    "        pattern_index_mapping = {match: f\"{'_'.join(match.split('_')[:-1])} {next(token_cycle)}\"\n",
    "                for match in set(self.pattern_index.findall(ampl))\n",
    "            }\n",
    "        for key, value in pattern_index_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "            \n",
    "        return temp_ampl\n",
    "    \n",
    "    def src_tokenize(self, ampl, seed):\n",
    "        \"\"\"Tokenize source expression, optionally applying replacements.\"\"\"\n",
    "        temp_ampl = self.src_replace(ampl, seed) if self.to_replace else ampl\n",
    "        temp_ampl = temp_ampl.replace('\\\\\\\\', '\\\\').replace('\\\\', ' \\\\ ').replace('%', '')\n",
    "\n",
    "        temp_ampl = self.pattern_underscore_curly.sub(lambda match: f' {match.group(0)} ', temp_ampl)\n",
    "\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_ampl = pattern.sub(f' {symbol} ', temp_ampl)\n",
    "        \n",
    "        temp_ampl = re.sub(r' {2,}', ' ', temp_ampl)\n",
    "        return [token for token in self.split_expression(temp_ampl) if token]\n",
    "\n",
    "    def tgt_tokenize(self, sqampl):\n",
    "        \"\"\"Tokenize target expression.\"\"\"\n",
    "        sqampl = self.remove_whitespace(sqampl)\n",
    "        temp_sqampl = sqampl\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_sqampl = pattern.sub(f' {symbol} ', temp_sqampl)\n",
    "        \n",
    "        for pattern in [self.pattern_reg_prop, self.pattern_mass, self.pattern_s]:\n",
    "            temp_sqampl = pattern.sub(lambda match: f' {match.group(0)} ', temp_sqampl)\n",
    "        \n",
    "        temp_sqampl = re.sub(r' {2,}', ' ', temp_sqampl)\n",
    "        return [token for token in self.split_expression(temp_sqampl) if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token indices\n",
    "BOS_IDX = 0  # Beginning of Sequence\n",
    "PAD_IDX = 1  # Padding\n",
    "EOS_IDX = 2  # End of Sequence\n",
    "UNK_IDX = 3  # Unknown Token\n",
    "SEP_IDX = 4  # Separator Token\n",
    "\n",
    "# Special token symbols\n",
    "SPECIAL_SYMBOLS = ['<START>', '<PAD>', '<END>', '<UNK>', '<SEP>']\n",
    "import re\n",
    "import random\n",
    "from itertools import cycle\n",
    "tokenizer = Tokenizer(df, 100, 100, SPECIAL_SYMBOLS, UNK_IDX, to_replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original expression: -i*e^2*gamma_{+%\\sigma_157721,%gam_166722,%eps_44575}*gamma_{%\\sigma_157721,%gam_166723,%del_106099}*e_{i_36289,%del_106099}(p_3)_v*e_{k_36277,%gam_166723}(p_1)_v^(*)*mu_{l_36277,%gam_166722}(p_2)_v^(*)*mu_{j_36269,%eps_44575}(p_4)_v/(m_e^2 + (-2)*s_13 + s_33 + reg_prop)\n",
      "Tokenized expression: ['-', 'i', '*', 'e', '^', '2', '*', 'gamma_{', '+', '%', '\\\\', 'sigma_157721', ',', '%', 'gam_166722', ',', '%', 'eps_44575', '}', '*', 'gamma_{', '%', '\\\\', 'sigma_157721', ',', '%', 'gam_166723', ',', '%', 'del_106099', '}', '*', 'e_{', 'i_36289', ',', '%', 'del_106099', '}', '(', 'p_3', ')', '_v', '*', 'e_{', 'k_36277', ',', '%', 'gam_166723', '}', '(', 'p_1', ')', '_v', '^', '(', '*', ')', '*', 'mu_{', 'l_36277', ',', '%', 'gam_166722', '}', '(', 'p_2', ')', '_v', '^', '(', '*', ')', '*', 'mu_{', 'j_36269', ',', '%', 'eps_44575', '}', '(', 'p_4', ')', '_v/', '(', 'm_e', '^', '2', '+', '(', '-', '2', ')', '*', 's_13', '+', 's_33', '+', 'reg_prop', ')']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_expression(expr):\n",
    "    # Tokenize the expression\n",
    "    tokens = tokenizer.tgt_tokenize(expr)\n",
    "    return tokens\n",
    "\n",
    "# Example of how the tokenization works\n",
    "tokens = tokenize_expression(example_expr)\n",
    "print(\"Original expression:\", example_expr)\n",
    "print(\"Tokenized expression:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_indices(tokenizer, expressions, index_token_pool_size=50, momentum_token_pool_size=50):\n",
    "    # Function to replace indices with a new set of tokens for each expression\n",
    "    def replace_indices(token_list, index_map):\n",
    "        new_index = (f\"INDEX_{i}\" for i in range(index_token_pool_size))  # Local generator for new indices\n",
    "        new_tokens = []\n",
    "        for token in token_list:\n",
    "            if \"INDEX_\" in token:\n",
    "                if token not in index_map:\n",
    "                    try:\n",
    "                        index_map[token] = next(new_index)\n",
    "                    except StopIteration:\n",
    "                        # Handle the case where no more indices are available\n",
    "                        raise ValueError(\"Ran out of unique indices, increase token_pool_size\")\n",
    "                new_tokens.append(index_map[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def replace_momenta(token_list, index_map):\n",
    "        new_index = (f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size))  # Local generator for new indices\n",
    "        new_tokens = []\n",
    "        for token in token_list:\n",
    "            if \"MOMENTUM_\" in token:\n",
    "                if token not in index_map:\n",
    "                    try:\n",
    "                        index_map[token] = next(new_index)\n",
    "                    except StopIteration:\n",
    "                        # Handle the case where no more indices are available\n",
    "                        raise ValueError(\"Ran out of unique indices, increase momentum_token_pool_size\")\n",
    "                new_tokens.append(index_map[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    normalized_expressions = []\n",
    "    # Replace indices in each expression randomly\n",
    "    for expr in expressions:\n",
    "        toks = tokenizer.src_tokenize(expr,42)\n",
    "        print(toks)\n",
    "        normalized_expressions.append(replace_momenta(replace_indices(toks, {}), {}))\n",
    "\n",
    "    return normalized_expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', 'i', '*', 'e', '^', '2', '*', 'gamma_{', '+', '\\\\', 'INDEX_31', ',', 'INDEX_35', ',', 'INDEX_28', '}', '*', 'gamma_{', '\\\\', 'INDEX_31', ',', 'INDEX_3', ',', 'INDEX_94', '}', '*', 'e_{', 'MOMENTUM_27', ',', 'INDEX_94', '}', '(', 'p_3', ')', '_v', '*', 'e_{', 'MOMENTUM_40', ',', 'INDEX_3', '}', '(', 'p_1', ')', '_v', '^', '(', '*', ')', '*', 'mu_{', 'MOMENTUM_72', ',', 'INDEX_35', '}', '(', 'p_2', ')', '_v', '^', '(', '*', ')', '*', 'mu_{', 'MOMENTUM_91', ',', 'INDEX_28', '}', '(', 'p_4', ')', '_v/', '(', 'm_e', '^', '2', '+', '(', '-', '2', ')', '*', 's_13', '+', 's_33', '+', 'reg_prop', ')']\n",
      "Original expression: -i*e^2*gamma_{+%\\sigma_157721,%gam_166722,%eps_44575}*gamma_{%\\sigma_157721,%gam_166723,%del_106099}*e_{i_36289,%del_106099}(p_3)_v*e_{k_36277,%gam_166723}(p_1)_v^(*)*mu_{l_36277,%gam_166722}(p_2)_v^(*)*mu_{j_36269,%eps_44575}(p_4)_v/(m_e^2 + (-2)*s_13 + s_33 + reg_prop)\n",
      "Normalized expression: ['-', 'i', '*', 'e', '^', '2', '*', 'gamma_{', '+', '\\\\', 'INDEX_0', ',', 'INDEX_1', ',', 'INDEX_2', '}', '*', 'gamma_{', '\\\\', 'INDEX_0', ',', 'INDEX_3', ',', 'INDEX_4', '}', '*', 'e_{', 'MOMENTUM_0', ',', 'INDEX_4', '}', '(', 'p_3', ')', '_v', '*', 'e_{', 'MOMENTUM_1', ',', 'INDEX_3', '}', '(', 'p_1', ')', '_v', '^', '(', '*', ')', '*', 'mu_{', 'MOMENTUM_2', ',', 'INDEX_1', '}', '(', 'p_2', ')', '_v', '^', '(', '*', ')', '*', 'mu_{', 'MOMENTUM_3', ',', 'INDEX_2', '}', '(', 'p_4', ')', '_v/', '(', 'm_e', '^', '2', '+', '(', '-', '2', ')', '*', 's_13', '+', 's_33', '+', 'reg_prop', ')']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "notmalized_tokens = normalize_indices(tokenizer, [example_expr])\n",
    "print(\"Original expression:\", example_expr)\n",
    "print(\"Normalized expression:\", notmalized_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original expression: -i*e^2*gamma_{+%\\sigma_157721,%gam_166722,%eps_44575}*gamma_{%\\sigma_157721,%gam_166723,%del_106099}*e_{i_36289,%del_106099}(p_3)_v*e_{k_36277,%gam_166723}(p_1)_v^(*)*mu_{l_36277,%gam_166722}(p_2)_v^(*)*mu_{j_36269,%eps_44575}(p_4)_v/(m_e^2 + (-2)*s_13 + s_33 + reg_prop)\n",
      "Normalized expression: ['-', 'i', '*', 'e', '^', '2', '*', 'gamma_{', '+', '\\\\', 'INDEX_0', ',', 'INDEX_1', ',', 'INDEX_2', '}', '*', 'gamma_{', '\\\\', 'INDEX_0', ',', 'INDEX_3', ',', 'INDEX_4', '}', '*', 'e_{', 'MOMENTUM_0', ',', 'INDEX_4', '}', '(', 'p_3', ')', '_v', '*', 'e_{', 'MOMENTUM_1', ',', 'INDEX_3', '}', '(', 'p_1', ')', '_v', '^', '(', '*', ')', '*', 'mu_{', 'MOMENTUM_2', ',', 'INDEX_1', '}', '(', 'p_2', ')', '_v', '^', '(', '*', ')', '*', 'mu_{', 'MOMENTUM_3', ',', 'INDEX_2', '}', '(', 'p_4', ')', '_v/', '(', 'm_e', '^', '2', '+', '(', '-', '2', ')', '*', 's_13', '+', 's_33', '+', 'reg_prop', ')']\n"
     ]
    }
   ],
   "source": [
    "example_expr = \"-i*e^2*gamma_{+%\\sigma_157721,%gam_166722,%eps_44575}*gamma_{%\\sigma_157721,%gam_166723,%del_106099}*e_{i_36289,%del_106099}(p_3)_v*e_{k_36277,%gam_166723}(p_1)_v^(*)*mu_{l_36277,%gam_166722}(p_2)_v^(*)*mu_{j_36269,%eps_44575}(p_4)_v/(m_e^2 + (-2)*s_13 + s_33 + reg_prop)\"\n",
    "normalized = normalize_indices(tokenizer, [example_expr])[0]\n",
    "print(\"Original expression:\", example_expr)\n",
    "print(\"Normalized expression:\", normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (15552, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>diagram</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>squared_amplitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_1:e(X_2), e(X_4),  OffShell A(V_1), V...</td>\n",
       "      <td>-1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_1...</td>\n",
       "      <td>2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_0:e(X_2), e(X_3),  OffShell A(V_0), V...</td>\n",
       "      <td>1/2*i*e^2*gamma_{+%\\sigma_172,%gam_162,%del_14...</td>\n",
       "      <td>2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_1:e(X_2),  OffShell e(X_4),  OffShell...</td>\n",
       "      <td>-1/2*i*e^2*gamma_{+%\\sigma_293,%gam_358,%gam_3...</td>\n",
       "      <td>2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_0:e(X_2), e(X_3),  OffShell A(V_0), V...</td>\n",
       "      <td>1/2*i*e^2*gamma_{+%\\sigma_301,%gam_377,%del_27...</td>\n",
       "      <td>2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_1:e(X_2), e(X_4),  OffShell A(V_1), V...</td>\n",
       "      <td>-i*e^2*gamma_{+%\\sigma_435,%gam_574,%gam_575}*...</td>\n",
       "      <td>8*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          event_type  \\\n",
       "0  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "1  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "2  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "3  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "4  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "\n",
       "                                             diagram  \\\n",
       "0  Vertex V_1:e(X_2), e(X_4),  OffShell A(V_1), V...   \n",
       "1  Vertex V_0:e(X_2), e(X_3),  OffShell A(V_0), V...   \n",
       "2  Vertex V_1:e(X_2),  OffShell e(X_4),  OffShell...   \n",
       "3  Vertex V_0:e(X_2), e(X_3),  OffShell A(V_0), V...   \n",
       "4  Vertex V_1:e(X_2), e(X_4),  OffShell A(V_1), V...   \n",
       "\n",
       "                                           amplitude  \\\n",
       "0  -1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_1...   \n",
       "1  1/2*i*e^2*gamma_{+%\\sigma_172,%gam_162,%del_14...   \n",
       "2  -1/2*i*e^2*gamma_{+%\\sigma_293,%gam_358,%gam_3...   \n",
       "3  1/2*i*e^2*gamma_{+%\\sigma_301,%gam_377,%del_27...   \n",
       "4  -i*e^2*gamma_{+%\\sigma_435,%gam_574,%gam_575}*...   \n",
       "\n",
       "                                   squared_amplitude  \n",
       "0  2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
       "1  2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...  \n",
       "2  2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
       "3  2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...  \n",
       "4  8*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = [f\"SYMBA - Test Data\\QED-2-to-2-diag-TreeLevel-{i}.txt\" for i in range(0, 10)]\n",
    "\n",
    "# Load the sample data\n",
    "df = load_data(file_paths)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original amplitude:\n",
      "-1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_146}*gamma_{%\\sigma_165,%gam_147,%del_137}*e_{i_3,%gam_146}(p_1)_u*e_{k_3,%del_137}(p_2)_u*e_{l_3,%gam_145}(p_3)_u^(*)*e_{i_5,%gam_147}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\n",
      "\n",
      "Normalized amplitude:\n",
      "-1/2*i*e^2*gamma_{+%\\sigma_1,%gam_1,%gam_2}*gamma_{%\\sigma_1,%gam_3,%del_1}*e_{i_3,%gam_2}(p_1)_u*e_{k_3,%del_1}(p_2)_u*e_{l_3,%gam_1}(p_3)_u^(*)*e_{i_5,%gam_3}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\n"
     ]
    }
   ],
   "source": [
    "# Normalize indices in amplitudes and squared amplitudes\n",
    "df['normalized_amplitude'] = df['amplitude'].apply(normalize_indices)\n",
    "df['normalized_squared_amplitude'] = df['squared_amplitude'].apply(normalize_indices)\n",
    "\n",
    "# Display an example of normalization\n",
    "print(\"Original amplitude:\")\n",
    "print(df['amplitude'].iloc[0])\n",
    "print(\"\\nNormalized amplitude:\")\n",
    "print(df['normalized_amplitude'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized amplitude:\n",
      "-1/2*i*e^2*gamma_{+%\\sigma_1,%gam_1,%gam_2}*gamma_{%\\sigma_1,%gam_3,%del_1}*e_{i_3,%gam_2}(p_1)_u*e_{k_3,%del_1}(p_2)_u*e_{l_3,%gam_1}(p_3)_u^(*)*e_{i_5,%gam_3}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\n",
      "\n",
      "Tokenized amplitude (first 20 tokens):\n",
      "['-', '1/2', '*', 'i', '*', 'e', '^', '2', '*', 'gamma', '_{+%\\\\sigma_1,%gam_1,%gam_2}', '*', 'gamma', '_{%\\\\sigma_1,%gam_3,%del_1}', '*', 'e', '_{i_3,%gam_2}', '(', 'p_1', ')']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize normalized expressions\n",
    "df['tokenized_amplitude'] = df['normalized_amplitude'].apply(tokenize_expression)\n",
    "df['tokenized_squared_amplitude'] = df['normalized_squared_amplitude'].apply(tokenize_expression)\n",
    "\n",
    "# Display an example of tokenization\n",
    "print(\"Normalized amplitude:\")\n",
    "print(df['normalized_amplitude'].iloc[0])\n",
    "print(\"\\nTokenized amplitude (first 20 tokens):\")\n",
    "print(df['tokenized_amplitude'].iloc[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 12441\n",
      "Validation set size: 1555\n",
      "Test set size: 1556\n"
     ]
    }
   ],
   "source": [
    "# Split into train, validation, and test sets (80-10-10)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 44416\n",
      "Most common tokens: [('*', 575442), ('(', 285768), (')', 285768), ('+', 272448), ('2', 215021), ('^', 198555), ('-', 168857), ('e', 70464), ('reg_prop', 62784), ('gamma', 58752)]\n"
     ]
    }
   ],
   "source": [
    "# Analyze token distribution\n",
    "all_tokens = []\n",
    "for tokens in df['tokenized_amplitude'] + df['tokenized_squared_amplitude']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "print(f\"Total unique tokens: {len(token_counts)}\")\n",
    "print(f\"Most common tokens: {token_counts.most_common(10)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump into a pickle file\n",
    "outout_dir = \"data\"\n",
    "os.makedirs(outout_dir, exist_ok=True)\n",
    "train_df.to_pickle(os.path.join(outout_dir, \"train.pkl\"))\n",
    "val_df.to_pickle(os.path.join(outout_dir, \"val.pkl\"))\n",
    "test_df.to_pickle(os.path.join(outout_dir, \"test.pkl\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
