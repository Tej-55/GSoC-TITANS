2025-03-09 11:58:34.786730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-09 11:58:34.968064: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-09 11:58:35.018931: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
tokenizer_config.json: 100%|███████████████| 2.32k/2.32k [00:00<00:00, 21.4MB/s]
spiece.model: 100%|██████████████████████████| 792k/792k [00:00<00:00, 14.1MB/s]
tokenizer.json: 100%|██████████████████████| 1.39M/1.39M [00:00<00:00, 12.3MB/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
config.json: 100%|█████████████████████████| 1.21k/1.21k [00:00<00:00, 12.7MB/s]
model.safetensors: 100%|██████████████████████| 242M/242M [00:01<00:00, 237MB/s]
generation_config.json: 100%|██████████████████| 147/147 [00:00<00:00, 1.55MB/s]
Vocabulary size after adding special tokens: 32106
Special tokens added: ['*', '/', '+', '-', '^', '(', ')', '{', '}', '_', 'gamma', 'sigma', 'e^2']
Number of training batches: 1556
Number of validation batches: 195
Number of test batches: 195
Using device: cuda
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
Epoch 1/2 | Batch 10/1556 | Loss: 6.2553
Epoch 1/2 | Batch 20/1556 | Loss: 2.6500
Epoch 1/2 | Batch 30/1556 | Loss: 3.4242
Epoch 1/2 | Batch 40/1556 | Loss: 1.8961
Epoch 1/2 | Batch 50/1556 | Loss: 1.1764
Epoch 1/2 | Batch 60/1556 | Loss: 1.9290
Epoch 1/2 | Batch 70/1556 | Loss: 1.8114
Epoch 1/2 | Batch 80/1556 | Loss: 1.5301
Epoch 1/2 | Batch 90/1556 | Loss: 1.2720
Epoch 1/2 | Batch 100/1556 | Loss: 1.1924
Epoch 1/2 | Batch 110/1556 | Loss: 1.0708
Epoch 1/2 | Batch 120/1556 | Loss: 1.4309
Epoch 1/2 | Batch 130/1556 | Loss: 0.8862
Epoch 1/2 | Batch 140/1556 | Loss: 1.0003
Epoch 1/2 | Batch 150/1556 | Loss: 1.0274
Epoch 1/2 | Batch 160/1556 | Loss: 0.8415
Epoch 1/2 | Batch 170/1556 | Loss: 1.2855
Epoch 1/2 | Batch 180/1556 | Loss: 1.1119
Epoch 1/2 | Batch 190/1556 | Loss: 0.8943
Epoch 1/2 | Batch 200/1556 | Loss: 0.8400
Epoch 1/2 | Batch 210/1556 | Loss: 0.5807
Epoch 1/2 | Batch 220/1556 | Loss: 0.9233
Epoch 1/2 | Batch 230/1556 | Loss: 0.6234
Epoch 1/2 | Batch 240/1556 | Loss: 0.7000
Epoch 1/2 | Batch 250/1556 | Loss: 0.7243
Epoch 1/2 | Batch 260/1556 | Loss: 0.8486
Epoch 1/2 | Batch 270/1556 | Loss: 0.7895
Epoch 1/2 | Batch 280/1556 | Loss: 0.4918
Epoch 1/2 | Batch 290/1556 | Loss: 0.6017
Epoch 1/2 | Batch 300/1556 | Loss: 0.5632
Epoch 1/2 | Batch 310/1556 | Loss: 0.5844
Epoch 1/2 | Batch 320/1556 | Loss: 0.7519
Epoch 1/2 | Batch 330/1556 | Loss: 0.3570
Epoch 1/2 | Batch 340/1556 | Loss: 0.7282
Epoch 1/2 | Batch 350/1556 | Loss: 0.5133
Epoch 1/2 | Batch 360/1556 | Loss: 0.4661
Epoch 1/2 | Batch 370/1556 | Loss: 0.6915
Epoch 1/2 | Batch 380/1556 | Loss: 0.5806
Epoch 1/2 | Batch 390/1556 | Loss: 0.7027
Epoch 1/2 | Batch 400/1556 | Loss: 0.4181
Epoch 1/2 | Batch 410/1556 | Loss: 0.4508
Epoch 1/2 | Batch 420/1556 | Loss: 0.4808
Epoch 1/2 | Batch 430/1556 | Loss: 0.5005
Epoch 1/2 | Batch 440/1556 | Loss: 0.3595
Epoch 1/2 | Batch 450/1556 | Loss: 0.4032
Epoch 1/2 | Batch 460/1556 | Loss: 0.4086
Epoch 1/2 | Batch 470/1556 | Loss: 0.4877
Epoch 1/2 | Batch 480/1556 | Loss: 0.3318
Epoch 1/2 | Batch 490/1556 | Loss: 0.2387
Epoch 1/2 | Batch 500/1556 | Loss: 0.3511
Epoch 1/2 | Batch 510/1556 | Loss: 0.5816
Epoch 1/2 | Batch 520/1556 | Loss: 0.4581
Epoch 1/2 | Batch 530/1556 | Loss: 0.3615
Epoch 1/2 | Batch 540/1556 | Loss: 0.4918
Epoch 1/2 | Batch 550/1556 | Loss: 0.2164
Epoch 1/2 | Batch 560/1556 | Loss: 0.4858
Epoch 1/2 | Batch 570/1556 | Loss: 0.4193
Epoch 1/2 | Batch 580/1556 | Loss: 0.5973
Epoch 1/2 | Batch 590/1556 | Loss: 0.4833
Epoch 1/2 | Batch 600/1556 | Loss: 0.2533
Epoch 1/2 | Batch 610/1556 | Loss: 0.3505
Epoch 1/2 | Batch 620/1556 | Loss: 0.3604
Epoch 1/2 | Batch 630/1556 | Loss: 0.5175
Epoch 1/2 | Batch 640/1556 | Loss: 0.2701
Epoch 1/2 | Batch 650/1556 | Loss: 0.2872
Epoch 1/2 | Batch 660/1556 | Loss: 0.4755
Epoch 1/2 | Batch 670/1556 | Loss: 0.3389
Epoch 1/2 | Batch 680/1556 | Loss: 0.3203
Epoch 1/2 | Batch 690/1556 | Loss: 0.3405
Epoch 1/2 | Batch 700/1556 | Loss: 0.3353
Epoch 1/2 | Batch 710/1556 | Loss: 0.2516
Epoch 1/2 | Batch 720/1556 | Loss: 0.2738
Epoch 1/2 | Batch 730/1556 | Loss: 0.2244
Epoch 1/2 | Batch 740/1556 | Loss: 0.2560
Epoch 1/2 | Batch 750/1556 | Loss: 0.3056
Epoch 1/2 | Batch 760/1556 | Loss: 0.1780
Epoch 1/2 | Batch 770/1556 | Loss: 0.3524
Epoch 1/2 | Batch 780/1556 | Loss: 0.2896
Epoch 1/2 | Batch 790/1556 | Loss: 0.1809
Epoch 1/2 | Batch 800/1556 | Loss: 0.1551
Epoch 1/2 | Batch 810/1556 | Loss: 0.3721
Epoch 1/2 | Batch 820/1556 | Loss: 0.2258
Epoch 1/2 | Batch 830/1556 | Loss: 0.3276
Epoch 1/2 | Batch 840/1556 | Loss: 0.2992
Epoch 1/2 | Batch 850/1556 | Loss: 0.2593
Epoch 1/2 | Batch 860/1556 | Loss: 0.1392
Epoch 1/2 | Batch 870/1556 | Loss: 0.2446
Epoch 1/2 | Batch 880/1556 | Loss: 0.2644
Epoch 1/2 | Batch 890/1556 | Loss: 0.2537
Epoch 1/2 | Batch 900/1556 | Loss: 0.1516
Epoch 1/2 | Batch 910/1556 | Loss: 0.2369
Epoch 1/2 | Batch 920/1556 | Loss: 0.2836
Epoch 1/2 | Batch 930/1556 | Loss: 0.3311
Epoch 1/2 | Batch 940/1556 | Loss: 0.2532
Epoch 1/2 | Batch 950/1556 | Loss: 0.3102
Epoch 1/2 | Batch 960/1556 | Loss: 0.2203
Epoch 1/2 | Batch 970/1556 | Loss: 0.2015
Epoch 1/2 | Batch 980/1556 | Loss: 0.2636
Epoch 1/2 | Batch 990/1556 | Loss: 0.2204
Epoch 1/2 | Batch 1000/1556 | Loss: 0.2050
Epoch 1/2 | Batch 1010/1556 | Loss: 0.1214
Epoch 1/2 | Batch 1020/1556 | Loss: 0.2015
Epoch 1/2 | Batch 1030/1556 | Loss: 0.3788
Epoch 1/2 | Batch 1040/1556 | Loss: 0.2991
Epoch 1/2 | Batch 1050/1556 | Loss: 0.1199
Epoch 1/2 | Batch 1060/1556 | Loss: 0.1940
Epoch 1/2 | Batch 1070/1556 | Loss: 0.2134
Epoch 1/2 | Batch 1080/1556 | Loss: 0.1291
Epoch 1/2 | Batch 1090/1556 | Loss: 0.1883
Epoch 1/2 | Batch 1100/1556 | Loss: 0.2148
Epoch 1/2 | Batch 1110/1556 | Loss: 0.2505
Epoch 1/2 | Batch 1120/1556 | Loss: 0.3183
Epoch 1/2 | Batch 1130/1556 | Loss: 0.2405
Epoch 1/2 | Batch 1140/1556 | Loss: 0.1782
Epoch 1/2 | Batch 1150/1556 | Loss: 0.2513
Epoch 1/2 | Batch 1160/1556 | Loss: 0.3344
Epoch 1/2 | Batch 1170/1556 | Loss: 0.3057
Epoch 1/2 | Batch 1180/1556 | Loss: 0.2176
Epoch 1/2 | Batch 1190/1556 | Loss: 0.1751
Epoch 1/2 | Batch 1200/1556 | Loss: 0.1166
Epoch 1/2 | Batch 1210/1556 | Loss: 0.2499
Epoch 1/2 | Batch 1220/1556 | Loss: 0.1612
Epoch 1/2 | Batch 1230/1556 | Loss: 0.1589
Epoch 1/2 | Batch 1240/1556 | Loss: 0.2362
Epoch 1/2 | Batch 1250/1556 | Loss: 0.3156
Epoch 1/2 | Batch 1260/1556 | Loss: 0.2008
Epoch 1/2 | Batch 1270/1556 | Loss: 0.1733
Epoch 1/2 | Batch 1280/1556 | Loss: 0.2122
Epoch 1/2 | Batch 1290/1556 | Loss: 0.1293
Epoch 1/2 | Batch 1300/1556 | Loss: 0.1642
Epoch 1/2 | Batch 1310/1556 | Loss: 0.1569
Epoch 1/2 | Batch 1320/1556 | Loss: 0.1648
Epoch 1/2 | Batch 1330/1556 | Loss: 0.2435
Epoch 1/2 | Batch 1340/1556 | Loss: 0.2397
Epoch 1/2 | Batch 1350/1556 | Loss: 0.2299
Epoch 1/2 | Batch 1360/1556 | Loss: 0.1667
Epoch 1/2 | Batch 1370/1556 | Loss: 0.1220
Epoch 1/2 | Batch 1380/1556 | Loss: 0.2698
Epoch 1/2 | Batch 1390/1556 | Loss: 0.1456
Epoch 1/2 | Batch 1400/1556 | Loss: 0.2124
Epoch 1/2 | Batch 1410/1556 | Loss: 0.2007
Epoch 1/2 | Batch 1420/1556 | Loss: 0.1553
Epoch 1/2 | Batch 1430/1556 | Loss: 0.2221
Epoch 1/2 | Batch 1440/1556 | Loss: 0.1678
Epoch 1/2 | Batch 1450/1556 | Loss: 0.1949
Epoch 1/2 | Batch 1460/1556 | Loss: 0.0839
Epoch 1/2 | Batch 1470/1556 | Loss: 0.1847
Epoch 1/2 | Batch 1480/1556 | Loss: 0.2046
Epoch 1/2 | Batch 1490/1556 | Loss: 0.1386
Epoch 1/2 | Batch 1500/1556 | Loss: 0.1163
Epoch 1/2 | Batch 1510/1556 | Loss: 0.1408
Epoch 1/2 | Batch 1520/1556 | Loss: 0.2933
Epoch 1/2 | Batch 1530/1556 | Loss: 0.2183
Epoch 1/2 | Batch 1540/1556 | Loss: 0.0889
Epoch 1/2 | Batch 1550/1556 | Loss: 0.1714
Epoch 1/2
Train loss: 0.5151
Validation loss: 0.1228
--------------------------------------------------
Epoch 2/2 | Batch 10/1556 | Loss: 0.2361
Epoch 2/2 | Batch 20/1556 | Loss: 0.2739
Epoch 2/2 | Batch 30/1556 | Loss: 0.1458
Epoch 2/2 | Batch 40/1556 | Loss: 0.1688
Epoch 2/2 | Batch 50/1556 | Loss: 0.1628
Epoch 2/2 | Batch 60/1556 | Loss: 0.2828
Epoch 2/2 | Batch 70/1556 | Loss: 0.2875
Epoch 2/2 | Batch 80/1556 | Loss: 0.1397
Epoch 2/2 | Batch 90/1556 | Loss: 0.1994
Epoch 2/2 | Batch 100/1556 | Loss: 0.2152
Epoch 2/2 | Batch 110/1556 | Loss: 0.1137
Epoch 2/2 | Batch 120/1556 | Loss: 0.2445
Epoch 2/2 | Batch 130/1556 | Loss: 0.2489
Epoch 2/2 | Batch 140/1556 | Loss: 0.2427
Epoch 2/2 | Batch 150/1556 | Loss: 0.1382
Epoch 2/2 | Batch 160/1556 | Loss: 0.1160
Epoch 2/2 | Batch 170/1556 | Loss: 0.1741
Epoch 2/2 | Batch 180/1556 | Loss: 0.1313
Epoch 2/2 | Batch 190/1556 | Loss: 0.1554
Epoch 2/2 | Batch 200/1556 | Loss: 0.1015
Epoch 2/2 | Batch 210/1556 | Loss: 0.2315
Epoch 2/2 | Batch 220/1556 | Loss: 0.0813
Epoch 2/2 | Batch 230/1556 | Loss: 0.1544
Epoch 2/2 | Batch 240/1556 | Loss: 0.1318
Epoch 2/2 | Batch 250/1556 | Loss: 0.2000
Epoch 2/2 | Batch 260/1556 | Loss: 0.1559
Epoch 2/2 | Batch 270/1556 | Loss: 0.1231
Epoch 2/2 | Batch 280/1556 | Loss: 0.0707
Epoch 2/2 | Batch 290/1556 | Loss: 0.1835
Epoch 2/2 | Batch 300/1556 | Loss: 0.1137
Epoch 2/2 | Batch 310/1556 | Loss: 0.1059
Epoch 2/2 | Batch 320/1556 | Loss: 0.1187
Epoch 2/2 | Batch 330/1556 | Loss: 0.1605
Epoch 2/2 | Batch 340/1556 | Loss: 0.0874
Epoch 2/2 | Batch 350/1556 | Loss: 0.2103
Epoch 2/2 | Batch 360/1556 | Loss: 0.1361
Epoch 2/2 | Batch 370/1556 | Loss: 0.2048
Epoch 2/2 | Batch 380/1556 | Loss: 0.2051
Epoch 2/2 | Batch 390/1556 | Loss: 0.1895
Epoch 2/2 | Batch 400/1556 | Loss: 0.1504
Epoch 2/2 | Batch 410/1556 | Loss: 0.0619
Epoch 2/2 | Batch 420/1556 | Loss: 0.1640
Epoch 2/2 | Batch 430/1556 | Loss: 0.2403
Epoch 2/2 | Batch 440/1556 | Loss: 0.1199
Epoch 2/2 | Batch 450/1556 | Loss: 0.1660
Epoch 2/2 | Batch 460/1556 | Loss: 0.1000
Epoch 2/2 | Batch 470/1556 | Loss: 0.1673
Epoch 2/2 | Batch 480/1556 | Loss: 0.0629
Epoch 2/2 | Batch 490/1556 | Loss: 0.1353
Epoch 2/2 | Batch 500/1556 | Loss: 0.1251
Epoch 2/2 | Batch 510/1556 | Loss: 0.1340
Epoch 2/2 | Batch 520/1556 | Loss: 0.1414
Epoch 2/2 | Batch 530/1556 | Loss: 0.1799
Epoch 2/2 | Batch 540/1556 | Loss: 0.0696
Epoch 2/2 | Batch 550/1556 | Loss: 0.1243
Epoch 2/2 | Batch 560/1556 | Loss: 0.0618
Epoch 2/2 | Batch 570/1556 | Loss: 0.1878
Epoch 2/2 | Batch 580/1556 | Loss: 0.1181
Epoch 2/2 | Batch 590/1556 | Loss: 0.1816
Epoch 2/2 | Batch 600/1556 | Loss: 0.0989
Epoch 2/2 | Batch 610/1556 | Loss: 0.0847
Epoch 2/2 | Batch 620/1556 | Loss: 0.1200
Epoch 2/2 | Batch 630/1556 | Loss: 0.1426
Epoch 2/2 | Batch 640/1556 | Loss: 0.1515
Epoch 2/2 | Batch 650/1556 | Loss: 0.1399
Epoch 2/2 | Batch 660/1556 | Loss: 0.0980
Epoch 2/2 | Batch 670/1556 | Loss: 0.0842
Epoch 2/2 | Batch 680/1556 | Loss: 0.1907
Epoch 2/2 | Batch 690/1556 | Loss: 0.0991
Epoch 2/2 | Batch 700/1556 | Loss: 0.1803
Epoch 2/2 | Batch 710/1556 | Loss: 0.1423
Epoch 2/2 | Batch 720/1556 | Loss: 0.1133
Epoch 2/2 | Batch 730/1556 | Loss: 0.1127
Epoch 2/2 | Batch 740/1556 | Loss: 0.1530
Epoch 2/2 | Batch 750/1556 | Loss: 0.1478
Epoch 2/2 | Batch 760/1556 | Loss: 0.1473
Epoch 2/2 | Batch 770/1556 | Loss: 0.1979
Epoch 2/2 | Batch 780/1556 | Loss: 0.0857
Epoch 2/2 | Batch 790/1556 | Loss: 0.0818
Epoch 2/2 | Batch 800/1556 | Loss: 0.0816
Epoch 2/2 | Batch 810/1556 | Loss: 0.0750
Epoch 2/2 | Batch 820/1556 | Loss: 0.1287
Epoch 2/2 | Batch 830/1556 | Loss: 0.0667
Epoch 2/2 | Batch 840/1556 | Loss: 0.1489
Epoch 2/2 | Batch 850/1556 | Loss: 0.1483
Epoch 2/2 | Batch 860/1556 | Loss: 0.1804
Epoch 2/2 | Batch 870/1556 | Loss: 0.1122
Epoch 2/2 | Batch 880/1556 | Loss: 0.0916
Epoch 2/2 | Batch 890/1556 | Loss: 0.1158
Epoch 2/2 | Batch 900/1556 | Loss: 0.1051
Epoch 2/2 | Batch 910/1556 | Loss: 0.1124
Epoch 2/2 | Batch 920/1556 | Loss: 0.0870
Epoch 2/2 | Batch 930/1556 | Loss: 0.1106
Epoch 2/2 | Batch 940/1556 | Loss: 0.1036
Epoch 2/2 | Batch 950/1556 | Loss: 0.1432
Epoch 2/2 | Batch 960/1556 | Loss: 0.1285
Epoch 2/2 | Batch 970/1556 | Loss: 0.1134
Epoch 2/2 | Batch 980/1556 | Loss: 0.0705
Epoch 2/2 | Batch 990/1556 | Loss: 0.1593
Epoch 2/2 | Batch 1000/1556 | Loss: 0.1258
Epoch 2/2 | Batch 1010/1556 | Loss: 0.0842
Epoch 2/2 | Batch 1020/1556 | Loss: 0.0647
Epoch 2/2 | Batch 1030/1556 | Loss: 0.0638
Epoch 2/2 | Batch 1040/1556 | Loss: 0.1246
Epoch 2/2 | Batch 1050/1556 | Loss: 0.1463
Epoch 2/2 | Batch 1060/1556 | Loss: 0.0694
Epoch 2/2 | Batch 1070/1556 | Loss: 0.1043
Epoch 2/2 | Batch 1080/1556 | Loss: 0.2053
Epoch 2/2 | Batch 1090/1556 | Loss: 0.0895
Epoch 2/2 | Batch 1100/1556 | Loss: 0.1103
Epoch 2/2 | Batch 1110/1556 | Loss: 0.1116
Epoch 2/2 | Batch 1120/1556 | Loss: 0.0741
Epoch 2/2 | Batch 1130/1556 | Loss: 0.0943
Epoch 2/2 | Batch 1140/1556 | Loss: 0.0907
Epoch 2/2 | Batch 1150/1556 | Loss: 0.1589
Epoch 2/2 | Batch 1160/1556 | Loss: 0.0783
Epoch 2/2 | Batch 1170/1556 | Loss: 0.0959
Epoch 2/2 | Batch 1180/1556 | Loss: 0.0938
Epoch 2/2 | Batch 1190/1556 | Loss: 0.0907
Epoch 2/2 | Batch 1200/1556 | Loss: 0.1234
Epoch 2/2 | Batch 1210/1556 | Loss: 0.1565
Epoch 2/2 | Batch 1220/1556 | Loss: 0.0806
Epoch 2/2 | Batch 1230/1556 | Loss: 0.0872
Epoch 2/2 | Batch 1240/1556 | Loss: 0.1482
Epoch 2/2 | Batch 1250/1556 | Loss: 0.1324
Epoch 2/2 | Batch 1260/1556 | Loss: 0.1340
Epoch 2/2 | Batch 1270/1556 | Loss: 0.0600
Epoch 2/2 | Batch 1280/1556 | Loss: 0.1232
Epoch 2/2 | Batch 1290/1556 | Loss: 0.0820
Epoch 2/2 | Batch 1300/1556 | Loss: 0.1147
Epoch 2/2 | Batch 1310/1556 | Loss: 0.0663
Epoch 2/2 | Batch 1320/1556 | Loss: 0.0810
Epoch 2/2 | Batch 1330/1556 | Loss: 0.1152
Epoch 2/2 | Batch 1340/1556 | Loss: 0.0620
Epoch 2/2 | Batch 1350/1556 | Loss: 0.1302
Epoch 2/2 | Batch 1360/1556 | Loss: 0.1162
Epoch 2/2 | Batch 1370/1556 | Loss: 0.0436
Epoch 2/2 | Batch 1380/1556 | Loss: 0.1465
Epoch 2/2 | Batch 1390/1556 | Loss: 0.0717
Epoch 2/2 | Batch 1400/1556 | Loss: 0.0644
Epoch 2/2 | Batch 1410/1556 | Loss: 0.1082
Epoch 2/2 | Batch 1420/1556 | Loss: 0.1183
Epoch 2/2 | Batch 1430/1556 | Loss: 0.0800
Epoch 2/2 | Batch 1440/1556 | Loss: 0.0935
Epoch 2/2 | Batch 1450/1556 | Loss: 0.1334
Epoch 2/2 | Batch 1460/1556 | Loss: 0.0663
Epoch 2/2 | Batch 1470/1556 | Loss: 0.1133
Epoch 2/2 | Batch 1480/1556 | Loss: 0.0879
Epoch 2/2 | Batch 1490/1556 | Loss: 0.0741
Epoch 2/2 | Batch 1500/1556 | Loss: 0.1038
Epoch 2/2 | Batch 1510/1556 | Loss: 0.0911
Epoch 2/2 | Batch 1520/1556 | Loss: 0.1026
Epoch 2/2 | Batch 1530/1556 | Loss: 0.0391
Epoch 2/2 | Batch 1540/1556 | Loss: 0.0788
Epoch 2/2 | Batch 1550/1556 | Loss: 0.0453
Epoch 2/2
Train loss: 0.1217
Validation loss: 0.0486
--------------------------------------------------
Figure(1000x600)
Evaluated 5/195 batches
Evaluated 10/195 batches
Evaluated 15/195 batches
Evaluated 20/195 batches
Evaluated 25/195 batches
Evaluated 30/195 batches
Evaluated 35/195 batches
Evaluated 40/195 batches
Evaluated 45/195 batches
Evaluated 50/195 batches
Evaluated 55/195 batches
Evaluated 60/195 batches
Evaluated 65/195 batches
Evaluated 70/195 batches
Evaluated 75/195 batches
Evaluated 80/195 batches
Evaluated 85/195 batches
Evaluated 90/195 batches
Evaluated 95/195 batches
Evaluated 100/195 batches
Evaluated 105/195 batches
Evaluated 110/195 batches
Evaluated 115/195 batches
Evaluated 120/195 batches
Evaluated 125/195 batches
Evaluated 130/195 batches
Evaluated 135/195 batches
Evaluated 140/195 batches
Evaluated 145/195 batches
Evaluated 150/195 batches
Evaluated 155/195 batches
Evaluated 160/195 batches
Evaluated 165/195 batches
Evaluated 170/195 batches
Evaluated 175/195 batches
Evaluated 180/195 batches
Evaluated 185/195 batches
Evaluated 190/195 batches
Evaluated 195/195 batches
Test sequence accuracy: 0.1491

Sample predictions vs targets:

Example 1:
Prediction: - 1 / 4 * e ^ 4 * s _ 33 * ( 64 * m _ t ^ 2 + ( - 16 ) * s _ 34 ) * ( m _ t ^ 2 + - s _ 11 + 2 * s _...
Target: 4 * e ^ 4 * s _ 13 * s _ 23 * ( m _ t ^ 2 + - s _ 22 + 2 * s _ 23 + - reg _ prop ) ^ ( - 2 ) + 8 * i...
Correct: False

Example 2:
Prediction: - 1 / 2 * e ^ 4 * s _ 13 * s _ 24 * ( s _ 13 + - 1 / 2 * reg _ prop ) ^ ( - 2 ) + 2 / 81 * i * e ^ 2...
Target: e ^ 4 * s _ 14 * s _ 24 * ( s _ 13 + - 1 / 2 * reg _ prop ) ^ ( - 2 ) + 2 * i * e ^ 2 * ( i * e ^ 2 ...
Correct: False

Example 3:
Prediction: 4 / 81 * e ^ 4 * s _ 14 * s _ 23 * ( s _ 22 + ( - 2 ) * s _ 23 + reg _ prop ) ^ ( - 2 ) + - 128 / 81...
Target: 16 / 81 * e ^ 4 * ( 16 * m _ u ^ 2 * s _ 22 + ( - 4 ) * s _ 13 * s _ 22 + 8 * s _ 12 * s _ 23 ) * ( ...
Correct: False

Example 4:
Prediction: 4 / 9 * e ^ 4 * ( 16 * m _ e ^ 2 * m _ tt ^ 2 + 8 * m _ tt ^ 2 * s _ 12 + 8 * s _ 14 * s _ 23 + 8 * ...
Target: 4 / 9 * e ^ 4 * ( 16 * m _ e ^ 2 * m _ tt ^ 2 + 8 * m _ tt ^ 2 * s _ 12 + 8 * s _ 14 * s _ 23 + 8 * ...
Correct: True

Example 5:
Prediction: 4 / 9 * e ^ 4 * ( 16 * m _ c ^ 2 * m _ mu ^ 2 + 8 * m _ mu ^ 2 * s _ 12 + 8 * s _ 14 * s _ 23 + 8 * ...
Target: 4 / 9 * e ^ 4 * ( 16 * m _ c ^ 2 * m _ mu ^ 2 + 8 * m _ c ^ 2 * s _ 12 + 8 * s _ 14 * s _ 23 + 8 * s...
Correct: False
Model and tokenizer saved successfully!