{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and parse data from all files\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' : ')\n",
    "                if len(parts) == 4:\n",
    "                    event_type, diagram, amplitude, squared_amplitude = parts\n",
    "                    data.append({\n",
    "                        'event_type': event_type,\n",
    "                        'diagram': diagram,\n",
    "                        'amplitude': amplitude.strip(),\n",
    "                        'squared_amplitude': squared_amplitude.strip()\n",
    "                    })\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_indices(expr):\n",
    "    # Find all patterns like %something_number\n",
    "    pattern = r'(%[^_]+)_(\\d+)'\n",
    "    matches = re.findall(pattern, expr)\n",
    "    \n",
    "    # Get unique numeric indices for each variable type\n",
    "    var_indices = {}\n",
    "    for var_type, num_idx in matches:\n",
    "        if var_type not in var_indices:\n",
    "            var_indices[var_type] = set()\n",
    "        var_indices[var_type].add(num_idx)\n",
    "    \n",
    "    # Create mapping from original indices to normalized ones for each variable type\n",
    "    index_maps = {}\n",
    "    for var_type, indices in var_indices.items():\n",
    "        sorted_indices = sorted(indices, key=int)\n",
    "        index_maps[var_type] = {orig_idx: str(i+1) for i, orig_idx in enumerate(sorted_indices)}\n",
    "    \n",
    "    # Replace indices according to the mapping\n",
    "    def replace_match(m):\n",
    "        var_type, num_idx = m.groups()\n",
    "        return f\"{var_type}_{index_maps[var_type][num_idx]}\"\n",
    "    \n",
    "    normalized_expr = re.sub(pattern, replace_match, expr)\n",
    "    \n",
    "    return normalized_expr\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for mathematical expressions\n",
    "def tokenize_expression(expr):\n",
    "    # Define patterns for different token types\n",
    "    patterns = [\n",
    "        # Constants and numbers\n",
    "        r'(\\d+/\\d+|\\d+\\.\\d+|\\d+)',\n",
    "        # Variables with indices and special notations\n",
    "        r'([a-zA-Z]+(?:_[a-zA-Z0-9]+)?(?:\\^\\([*]\\))?)',\n",
    "        # Mathematical operators and symbols\n",
    "        r'([\\+\\-\\*/\\^\\(\\)\\[\\]\\{\\}])',\n",
    "        # Special symbols and groupings\n",
    "        r'(%[a-zA-Z]+_\\d+|_{[^}]+})'\n",
    "    ]\n",
    "    \n",
    "    # Combine patterns\n",
    "    combined_pattern = '|'.join(patterns)\n",
    "    tokens = re.findall(combined_pattern, expr)\n",
    "    \n",
    "    # Flatten and filter empty strings\n",
    "    tokens = [t for sublist in tokens for t in sublist if t]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original expression: -1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_146}*gamma_{%\\sigma_165,%gam_147,%del_137}*e_{i_3,%gam_146}(p_1)_u*e_{k_3,%del_137}(p_2)_u*e_{l_3,%gam_145}(p_3)_u^(*)*e_{i_5,%gam_147}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\n",
      "Tokenized expression: ['-', '1/2', '*', 'i', '*', 'e', '^', '2', '*', 'gamma', '_{+%\\\\sigma_165,%gam_145,%gam_146}', '*', 'gamma', '_{%\\\\sigma_165,%gam_147,%del_137}', '*', 'e', '_{i_3,%gam_146}', '(', 'p_1', ')', 'u', '*', 'e', '_{k_3,%del_137}', '(', 'p_2', ')', 'u', '*', 'e', '_{l_3,%gam_145}', '(', 'p_3', ')', 'u^(*)', '*', 'e', '_{i_5,%gam_147}', '(', 'p_4', ')', 'u^(*)', '/', '(', 'm_e', '^', '2', '+', '-', 's_13', '+', '1/2', '*', 'reg_prop', ')']\n"
     ]
    }
   ],
   "source": [
    "# Example of how the tokenization works\n",
    "example_expr = \"-1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_146}*gamma_{%\\sigma_165,%gam_147,%del_137}*e_{i_3,%gam_146}(p_1)_u*e_{k_3,%del_137}(p_2)_u*e_{l_3,%gam_145}(p_3)_u^(*)*e_{i_5,%gam_147}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\"\n",
    "tokens = tokenize_expression(example_expr)\n",
    "print(\"Original expression:\", example_expr)\n",
    "print(\"Tokenized expression:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original expression: -i*e^2*gamma_{+%\\sigma_157721,%gam_166722,%eps_44575}*gamma_{%\\sigma_157721,%gam_166723,%del_106099}*e_{i_36289,%del_106099}(p_3)_v*e_{k_36277,%gam_166723}(p_1)_v^(*)*mu_{l_36277,%gam_166722}(p_2)_v^(*)*mu_{j_36269,%eps_44575}(p_4)_v/(m_e^2 + (-2)*s_13 + s_33 + reg_prop)\n",
      "Normalized expression: -i*e^2*gamma_{+%\\sigma_1,%gam_1,%eps_1}*gamma_{%\\sigma_1,%gam_2,%del_1}*e_{i_36289,%del_1}(p_3)_v*e_{k_36277,%gam_2}(p_1)_v^(*)*mu_{l_36277,%gam_1}(p_2)_v^(*)*mu_{j_36269,%eps_1}(p_4)_v/(m_e^2 + (-2)*s_13 + s_33 + reg_prop)\n"
     ]
    }
   ],
   "source": [
    "example_expr = \"-i*e^2*gamma_{+%\\sigma_157721,%gam_166722,%eps_44575}*gamma_{%\\sigma_157721,%gam_166723,%del_106099}*e_{i_36289,%del_106099}(p_3)_v*e_{k_36277,%gam_166723}(p_1)_v^(*)*mu_{l_36277,%gam_166722}(p_2)_v^(*)*mu_{j_36269,%eps_44575}(p_4)_v/(m_e^2 + (-2)*s_13 + s_33 + reg_prop)\"\n",
    "normalized = normalize_indices(example_expr)\n",
    "print(\"Original expression:\", example_expr)\n",
    "print(\"Normalized expression:\", normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (15552, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>diagram</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>squared_amplitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_1:e(X_2), e(X_4),  OffShell A(V_1), V...</td>\n",
       "      <td>-1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_1...</td>\n",
       "      <td>2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_0:e(X_2), e(X_3),  OffShell A(V_0), V...</td>\n",
       "      <td>1/2*i*e^2*gamma_{+%\\sigma_172,%gam_162,%del_14...</td>\n",
       "      <td>2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_1:e(X_2),  OffShell e(X_4),  OffShell...</td>\n",
       "      <td>-1/2*i*e^2*gamma_{+%\\sigma_293,%gam_358,%gam_3...</td>\n",
       "      <td>2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_0:e(X_2), e(X_3),  OffShell A(V_0), V...</td>\n",
       "      <td>1/2*i*e^2*gamma_{+%\\sigma_301,%gam_377,%del_27...</td>\n",
       "      <td>2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...</td>\n",
       "      <td>Vertex V_1:e(X_2), e(X_4),  OffShell A(V_1), V...</td>\n",
       "      <td>-i*e^2*gamma_{+%\\sigma_435,%gam_574,%gam_575}*...</td>\n",
       "      <td>8*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          event_type  \\\n",
       "0  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "1  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "2  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "3  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "4  Interaction:  e_gam_239(X)^(*)  e_del_219(X)^(...   \n",
       "\n",
       "                                             diagram  \\\n",
       "0  Vertex V_1:e(X_2), e(X_4),  OffShell A(V_1), V...   \n",
       "1  Vertex V_0:e(X_2), e(X_3),  OffShell A(V_0), V...   \n",
       "2  Vertex V_1:e(X_2),  OffShell e(X_4),  OffShell...   \n",
       "3  Vertex V_0:e(X_2), e(X_3),  OffShell A(V_0), V...   \n",
       "4  Vertex V_1:e(X_2), e(X_4),  OffShell A(V_1), V...   \n",
       "\n",
       "                                           amplitude  \\\n",
       "0  -1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_1...   \n",
       "1  1/2*i*e^2*gamma_{+%\\sigma_172,%gam_162,%del_14...   \n",
       "2  -1/2*i*e^2*gamma_{+%\\sigma_293,%gam_358,%gam_3...   \n",
       "3  1/2*i*e^2*gamma_{+%\\sigma_301,%gam_377,%del_27...   \n",
       "4  -i*e^2*gamma_{+%\\sigma_435,%gam_574,%gam_575}*...   \n",
       "\n",
       "                                   squared_amplitude  \n",
       "0  2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
       "1  2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...  \n",
       "2  2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
       "3  2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...  \n",
       "4  8*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = [f\"SYMBA - Test Data\\QED-2-to-2-diag-TreeLevel-{i}.txt\" for i in range(0, 10)]\n",
    "\n",
    "# Load the sample data\n",
    "df = load_data(file_paths)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original amplitude:\n",
      "-1/2*i*e^2*gamma_{+%\\sigma_165,%gam_145,%gam_146}*gamma_{%\\sigma_165,%gam_147,%del_137}*e_{i_3,%gam_146}(p_1)_u*e_{k_3,%del_137}(p_2)_u*e_{l_3,%gam_145}(p_3)_u^(*)*e_{i_5,%gam_147}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\n",
      "\n",
      "Normalized amplitude:\n",
      "-1/2*i*e^2*gamma_{+%\\sigma_1,%gam_1,%gam_2}*gamma_{%\\sigma_1,%gam_3,%del_1}*e_{i_3,%gam_2}(p_1)_u*e_{k_3,%del_1}(p_2)_u*e_{l_3,%gam_1}(p_3)_u^(*)*e_{i_5,%gam_3}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\n"
     ]
    }
   ],
   "source": [
    "# Normalize indices in amplitudes and squared amplitudes\n",
    "df['normalized_amplitude'] = df['amplitude'].apply(normalize_indices)\n",
    "df['normalized_squared_amplitude'] = df['squared_amplitude'].apply(normalize_indices)\n",
    "\n",
    "# Display an example of normalization\n",
    "print(\"Original amplitude:\")\n",
    "print(df['amplitude'].iloc[0])\n",
    "print(\"\\nNormalized amplitude:\")\n",
    "print(df['normalized_amplitude'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized amplitude:\n",
      "-1/2*i*e^2*gamma_{+%\\sigma_1,%gam_1,%gam_2}*gamma_{%\\sigma_1,%gam_3,%del_1}*e_{i_3,%gam_2}(p_1)_u*e_{k_3,%del_1}(p_2)_u*e_{l_3,%gam_1}(p_3)_u^(*)*e_{i_5,%gam_3}(p_4)_u^(*)/(m_e^2 + -s_13 + 1/2*reg_prop)\n",
      "\n",
      "Tokenized amplitude (first 20 tokens):\n",
      "['-', '1/2', '*', 'i', '*', 'e', '^', '2', '*', 'gamma', '_{+%\\\\sigma_1,%gam_1,%gam_2}', '*', 'gamma', '_{%\\\\sigma_1,%gam_3,%del_1}', '*', 'e', '_{i_3,%gam_2}', '(', 'p_1', ')']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize normalized expressions\n",
    "df['tokenized_amplitude'] = df['normalized_amplitude'].apply(tokenize_expression)\n",
    "df['tokenized_squared_amplitude'] = df['normalized_squared_amplitude'].apply(tokenize_expression)\n",
    "\n",
    "# Display an example of tokenization\n",
    "print(\"Normalized amplitude:\")\n",
    "print(df['normalized_amplitude'].iloc[0])\n",
    "print(\"\\nTokenized amplitude (first 20 tokens):\")\n",
    "print(df['tokenized_amplitude'].iloc[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 12441\n",
      "Validation set size: 1555\n",
      "Test set size: 1556\n"
     ]
    }
   ],
   "source": [
    "# Split into train, validation, and test sets (80-10-10)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 44416\n",
      "Most common tokens: [('*', 575442), ('(', 285768), (')', 285768), ('+', 272448), ('2', 215021), ('^', 198555), ('-', 168857), ('e', 70464), ('reg_prop', 62784), ('gamma', 58752)]\n"
     ]
    }
   ],
   "source": [
    "# Analyze token distribution\n",
    "all_tokens = []\n",
    "for tokens in df['tokenized_amplitude'] + df['tokenized_squared_amplitude']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "print(f\"Total unique tokens: {len(token_counts)}\")\n",
    "print(f\"Most common tokens: {token_counts.most_common(10)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump into a pickle file\n",
    "outout_dir = \"data\"\n",
    "os.makedirs(outout_dir, exist_ok=True)\n",
    "train_df.to_pickle(os.path.join(outout_dir, \"train.pkl\"))\n",
    "val_df.to_pickle(os.path.join(outout_dir, \"val.pkl\"))\n",
    "test_df.to_pickle(os.path.join(outout_dir, \"test.pkl\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
